#  Copyright 2023 Amazon.com and its affiliates; all rights reserved.
#  This file is Amazon Web Services Content and may not be duplicated or distributed without permission.

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_models.bedrock import BedrockChat
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

supported_models = [
    "anthropic.claude-3-sonnet-20240229-v1:0",
    "anthropic.claude-3-haiku-20240307-v1:0",
    "anthropic.claude-3-opus-20240229-v1:0",
]


class Chatbot:
    def __init__(
        self,
        model_id="anthropic.claude-3-sonnet-20240229-v1:0",
        stop=None,
    ):
        if model_id not in supported_models:
            raise ValueError(f"model_id {model_id} is not supported")

        # Please check carefully with updated bedrock documents
        # https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/models
        self.max_token = 200000

        self.model_kwargs = {
            "temperature": 0.5,
            "top_p": 0.5,
            "max_tokens": 4096,
            "top_k": 50,
        }
        self.default_stop = ["\n\nHuman:"]

        self.stop = stop if stop is not None else self.default_stop

        self.engine = BedrockChat(
            model_id=model_id, streaming=True, model_kwargs=self.model_kwargs,
            region_name = 'us-west-2',
            credentials_profile_name='opus'
        )

    def ask_stream(
        self,
        input_data: dict,
        conversation_history: ConversationBufferMemory = ConversationBufferMemory(
            ai_prefix="assistant", human_prefix="user", return_messages=True
        ),
        verbose: bool = False,
        **kwargs,
    ):
        """Processes a stream of input by invoking the engine.

        Parameters
        ----------
        conversation_history: ConversationBufferMemory
            The conversation history
        input_data: dict
            The input message. It must have a `text` field
        verbose: boolean
            if the langchain shall show the detailed logging
        kwargs: dict
            Additional keyword arguments to pass to the engine.
            For example, you can pass in temperature to control the model's
            creativity.
        Returns
        -------
        str
            The response generated by the engine.

        Raises
        ------
        Any exceptions raised by the engine.
        """

        temp_stop = kwargs.get("stop", self.stop)

        system_message = """The following is a friendly conversation between a human and an AI. The AI is able to
        provide accurate information in a structured way. The AI is able to provide information in a clear and concise manner.If the AI does not know the answer to a question, it truthfully says it does not know.
         """

        if "image_data" in input_data:
            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(
                        content=system_message
                    ),  # The persistent system prompt
                    MessagesPlaceholder(
                        variable_name="history"
                    ),  # Where the memory will be stored.
                    (
                        "human",
                        [
                            {"type": "text", "text": "{input}"},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{input_data['image_data']}",
                                },
                            },
                        ],
                    ),
                ]
            )

        else:
            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(
                        content=system_message
                    ),  # The persistent system prompt
                    MessagesPlaceholder(
                        variable_name="history"
                    ),  # Where the memory will be stored.
                    HumanMessagePromptTemplate.from_template("{input}"),
                ]
            )

        chain = ConversationChain(
            llm=self.engine,
            memory=conversation_history,
            verbose=verbose,
            prompt=prompt,
        )

        return chain.predict(input=input_data["text"], stop=temp_stop)
