{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain\n",
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import time\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/data_connection/document_transformers/code_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nbformat\n",
    "import json\n",
    "\n",
    "# Extracts the python code from an .ipynb file from github\n",
    "def extract_python_code_from_ipynb(github_url,cell_type = \"code\"):\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    notebook_content = response.text\n",
    "\n",
    "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "    python_code = None\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == cell_type:\n",
    "          if not python_code:\n",
    "            python_code = cell.source\n",
    "          else:\n",
    "            python_code += \"\\n\" + cell.source\n",
    "\n",
    "    return python_code\n",
    "\n",
    "# Extracts the python code from an .py file from github\n",
    "def extract_python_code_from_py(github_url):\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    python_code = response.text\n",
    "\n",
    "    return python_code\n",
    "\n",
    "with open('./code_files_urls.txt') as f:\n",
    "    code_files_urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='!pip install huggingface-hub -Uqq\\n!pip install -Uqq sagemaker\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nlocal_model_path = Path(\"./bge-m3-model\")\\nlocal_model_path.mkdir(exist_ok=True)\\nmodel_name = \"BAAI/bge-m3\"\\ncommit_hash = \"4277867103fc67328e2033176de4387b85e9960f\"\\nsnapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)\\n!pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple -Uqq\\nfrom modelscope.hub.snapshot_download import snapshot_download\\nfrom pathlib import Path\\n\\nlocal_model_path = Path(\"./bge-zh-model\")\\n\\nlocal_model_path.mkdir(exist_ok=True)\\nmodel_name = \"Xorbits/bge-large-zh-v1.5\"\\ncommit_hash = \"v0.0.1\"\\n\\nsnapshot_download(model_name, revision=commit_hash, cache_dir=local_model_path)\\nimport sagemaker\\nfrom sagemaker import image_uris\\nimport boto3\\nimport os\\nimport time\\nimport json\\n\\nrole = sagemaker.get_execution_role()  # execution role for the endpoint\\nsess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\\nbucket = sess.default_bucket()  # bucket to house artifacts\\n\\nregion = sess._region_name\\naccount_id = sess.account_id()\\n\\ns3_client = boto3.client(\"s3\")\\nsm_client = boto3.client(\"sagemaker\")\\nsmr_client = boto3.client(\"sagemaker-runtime\")\\ns3_model_prefix = \"LLM-RAG/workshop/bge-m3-model\"  # folder where model checkpoint will go\\nif region in [\\'cn-north-1\\', \\'cn-northwest-1\\']:\\n    model_snapshot_path = f\\'{local_model_path}/{model_name}\\'\\nelse:\\n    model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\\ns3_code_prefix = \"LLM-RAG/workshop/bge_m3_deploy_code\"\\nprint(f\"s3_code_prefix: {s3_code_prefix}\")\\nprint(f\"model_snapshot_path: {model_snapshot_path}\")\\n!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}\\ninference_image_uri = (\\n    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\\n    \\n)\\n\\n#中国区需要替换为下面的image_uri\\nif region in [\\'cn-north-1\\', \\'cn-northwest-1\\']:\\n    inference_image_uri = (\\n        f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\\n    )\\n\\nprint(f\"Image going to be used is ---- > {inference_image_uri}\")\\n!mkdir -p bge_m3_deploy_code\\n%%writefile bge_m3_deploy_code/model.py\\nfrom djl_python import Input, Output\\nimport torch\\nimport logging\\nimport math\\nimport os\\nfrom FlagEmbedding import BGEM3FlagModel\\n\\ndevice = torch.device(\\'cuda:0\\' if torch.cuda.is_available() else \\'cpu\\')\\nprint(f\\'--device={device}\\')\\n\\ndef load_model(properties):\\n    tensor_parallel = properties[\"tensor_parallel_degree\"]\\n    model_location = properties[\\'model_dir\\']\\n    if \"model_id\" in properties:\\n        model_location = properties[\\'model_id\\']\\n    logging.info(f\"Loading model in {model_location}\")\\n\\n    model =  BGEM3FlagModel(model_location,use_fp16=True)\\n    \\n    return model\\n\\nmodel = None\\n\\ndef handle(inputs: Input):\\n    global model\\n    if not model:\\n        model = load_model(inputs.get_properties())\\n\\n    if inputs.is_empty():\\n        return None\\n    data = inputs.get_as_json()\\n    \\n    input_sentences = None\\n    inputs = data[\"inputs\"]\\n    if isinstance(inputs, list):\\n        input_sentences = inputs\\n    else:\\n        input_sentences =  [inputs]\\n        \\n    is_query = data.get(\"is_query\")\\n    max_length = data.get(\"max_length\",2048)\\n    instruction = data.get(\"instruction\")\\n    logging.info(f\"inputs: {input_sentences}\")\\n    logging.info(f\"is_query: {is_query}\")\\n    logging.info(f\"instruction: {instruction}\")\\n    \\n    if is_query and instruction:\\n        input_sentences = [ instruction + sent for sent in input_sentences ]\\n        \\n    sentence_embeddings =  model.encode(input_sentences,max_length=max_length)\\n        \\n    result = {\"sentence_embeddings\": sentence_embeddings[\\'dense_vecs\\']}\\n    return Output().add_as_json(result)\\nprint(f\"option.s3url ==> s3://{bucket}/{s3_model_prefix}/\")\\nimport os\\n\\nif not os.path.exists(\"bge_m3_deploy_code\"):\\n    os.mkdir(\"bge_m3_deploy_code\")\\n\\nwith open(\\'bge_m3_deploy_code/serving.properties\\', \\'w\\') as f:\\n    f.write(\"engine=Python\")\\n    f.write(\"\\\\n\")\\n    f.write(\"option.tensor_parallel_degree=1\")\\n    f.write(\"\\\\n\")\\n    f.write(f\"option.s3url=s3://{bucket}/{s3_model_prefix}/\")\\n%%writefile bge_m3_deploy_code/requirements.txt\\n-i https://pypi.tuna.tsinghua.edu.cn/simple\\nFlagEmbedding\\n%%writefile bge_m3_deploy_code/requirements.txt\\nFlagEmbedding\\n!rm s2e_model.tar.gz\\n!cd bge_m3_deploy_code && rm -rf \".ipynb_checkpoints\"\\n!tar czvf s2e_model.tar.gz bge_m3_deploy_code\\ns3_code_artifact = sess.upload_data(\"s2e_model.tar.gz\", bucket, s3_code_prefix)\\nprint(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\\nfrom sagemaker.utils import name_from_base\\nimport boto3\\n\\nmodel_name = name_from_base(\"bge-m3\") #Note: Need to specify model_name\\nprint(model_name)\\nprint(f\"Image going to be used is ---- > {inference_image_uri}\")\\n\\ncreate_model_response = sm_client.create_model(\\n    ModelName=model_name,\\n    ExecutionRoleArn=role,\\n    PrimaryContainer={\\n        \"Image\": inference_image_uri,\\n        \"ModelDataUrl\": s3_code_artifact\\n    },\\n    \\n)\\nmodel_arn = create_model_response[\"ModelArn\"]\\n\\nprint(f\"Created Model: {model_arn}\")\\nendpoint_config_name = f\"{model_name}-config\"\\nendpoint_name = f\"{model_name}-endpoint\"\\n\\nendpoint_config_response = sm_client.create_endpoint_config(\\n    EndpointConfigName=endpoint_config_name,\\n    ProductionVariants=[\\n        {\\n            \"VariantName\": \"variant1\",\\n            \"ModelName\": model_name,\\n            \"InstanceType\": \"ml.g4dn.xlarge\",\\n            \"InitialInstanceCount\": 1,\\n            # \"VolumeSizeInGB\" : 400,\\n            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\\n            \"ContainerStartupHealthCheckTimeoutInSeconds\": 5*60,\\n        },\\n    ],\\n)\\nendpoint_config_response\\ncreate_endpoint_response = sm_client.create_endpoint(\\n    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\\n)\\nprint(f\"Created Endpoint: {create_endpoint_response[\\'EndpointArn\\']}\")\\nimport time\\n\\nresp = sm_client.describe_endpoint(EndpointName=endpoint_name)\\nstatus = resp[\"EndpointStatus\"]\\nprint(\"Status: \" + status)\\n\\nwhile status == \"Creating\":\\n    time.sleep(60)\\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\\n    status = resp[\"EndpointStatus\"]\\n    print(\"Status: \" + status)\\n\\nprint(\"Arn: \" + resp[\"EndpointArn\"])\\nprint(\"Status: \" + status)\\ndef get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\\n    parameters = {\\n    }\\n\\n    response_model = sm_client.invoke_endpoint(\\n        EndpointName=endpoint_name,\\n        Body=json.dumps(\\n            {\\n                \"inputs\": questions,\\n                # \"is_query\": True,\\n                \"instruction\" :  \"Represent this sentence for searching relevant passages:\"\\n            }\\n        ),\\n        ContentType=\"application/json\",\\n    )\\n    # 中文instruction => 为这个句子生成表示以用于检索相关文章：\\n    json_str = response_model[\\'Body\\'].read().decode(\\'utf8\\')\\n    json_obj = json.loads(json_str)\\n    embeddings = json_obj[\\'sentence_embeddings\\']\\n    return embeddings\\nprompts1 = [\"How to compete with OCI\",\"如何与OCI竞争\",\"如何與OCI競爭\"]\\n\\nemb = get_vector_by_sm_endpoint(prompts1, smr_client, endpoint_name)\\nimport numpy as np\\ndef cos_sim(vector1, vector2):\\n    dot_product = np.dot(vector1, vector2)\\n    norm_v1 = np.linalg.norm(vector1)\\n    norm_v2 = np.linalg.norm(vector2)\\n    cos_sim = dot_product / (norm_v1 * norm_v2)\\n    return cos_sim\\ncos_sim(emb[1],emb[2])\\n\\n\\n\\n!aws sagemaker delete-endpoint --endpoint-name bge-large-en-2023-08-16-09-58-49-900-endpoint\\n!aws sagemaker delete-endpoint-config --endpoint-config-name bge-large-en-2023-08-16-09-58-49-900-config\\n!aws sagemaker delete-model --model-name bge-large-en-2023-08-16-09-58-49-900', metadata={'url': 'https://github.com/aws-samples/private-llm-qa-bot/blob/main/notebooks/embedding/bge_m3_deploy.ipynb', 'file_index': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_strings = []\n",
    "\n",
    "for i in range(0, len (code_files_urls)):\n",
    "    if code_files_urls[i].endswith(\".ipynb\"):\n",
    "        content = extract_python_code_from_ipynb(code_files_urls[i],\"code\")\n",
    "        doc = Document(page_content=content, metadata= {\"url\": code_files_urls[i], \"file_index\":i})\n",
    "        code_strings.append(doc)\n",
    "code_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GITHUB_TOKEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Crawls a GitHub repository and returns a list of all ipynb files in the repository\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawl_github_repo\u001b[39m(url,is_sub_dir,access_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mGITHUB_TOKEN\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     ignore_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sub_dir:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GITHUB_TOKEN' is not defined"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "#Crawls a GitHub repository and returns a list of all ipynb files in the repository\n",
    "def crawl_github_repo(url,is_sub_dir,access_token = f\"{GITHUB_TOKEN}\"):\n",
    "\n",
    "    ignore_list = ['__init__.py']\n",
    "\n",
    "    if not is_sub_dir:\n",
    "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
    "    else:\n",
    "        api_url = url\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\" \n",
    "                   }\n",
    "\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    files = []\n",
    "\n",
    "    contents = response.json()\n",
    "\n",
    "    for item in contents:\n",
    "        if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.py') or item['name'].endswith('.ipynb')):\n",
    "            files.append(item['html_url'])\n",
    "        elif item['type'] == 'dir' and not item['name'].startswith(\".\"):\n",
    "            sub_files = crawl_github_repo(item['url'],True)\n",
    "            time.sleep(.1)\n",
    "            files.extend(sub_files)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(\n",
    "    credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"This is a content of the document\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk code strings\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(code_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \\\n",
    "\"\"\"you are a professional programmer,\n",
    "please use the below reference code to response to user's request. \n",
    "<reference>\n",
    "{context}\n",
    "</reference>\n",
    "\n",
    "The user question can be a code completion request.\n",
    "Here is user's request:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                  model_kwargs={\"temperature\": 0.2,\n",
    "                                \"top_k\":250,\n",
    "                                \"max_tokens\": 1024,\n",
    "                                \"top_p\":0.95,\n",
    "                                # \"stop_sequences\":['</response>']\n",
    "                               },\n",
    "                  streaming=False,callbacks=[StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents('how deploy baichuan2 in sagemaker?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= ChatPromptTemplate.from_template(rag_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ({\"context\":retriever | format_docs,\"question\":RunnablePassthrough()}) | prompt |llm|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To deploy the Baichuan2 model on SageMaker, you can follow these steps:\\n\\n1. **Download the model checkpoint**\\n```python\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nlocal_model_path_name = \"./CSDC_buffer_baichuan2_13B_rag_4bits\"\\nmodel_hf_name = \"csdc-atl/buffer-baichuan2-13B-rag-4bits\"\\nmodel_name = model_hf_name.split(\\'/\\')[-1]\\n\\nlocal_model_path = Path(local_model_path_name)\\nlocal_model_path.mkdir(exist_ok=True)\\ncommit_hash = \\'107d6ef2ab9f77efc5d53ddab3d4a1621e531627\\'\\nsnapshot_download(repo_id=model_hf_name, revision=commit_hash, cache_dir=local_model_path)\\n```\\n\\n2. **Upload the model to S3**\\n```python\\nimport sagemaker\\nsess = sagemaker.session.Session()\\nbucket = sess.default_bucket()\\nregion = sess._region_name\\n\\ns3_model_prefix = f\"aigc-llm-models/{model_name}\"\\nmodel_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\\n\\n!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}\\n```\\n\\n3. **Set up the inference image URI**\\n```python\\ninference_image_uri = image_uris.retrieve(\\n    framework=\"djl-deepspeed\",\\n    region=sess.boto_session.region_name,\\n    version=\"0.25.0\"\\n)\\nprint(f\"Image going to be used is ---- > {inference_image_uri}\")\\n```\\n\\n4. **Create a deploy code directory and model.py file**\\n```python\\nlocal_code_dir = s3_code_prefix.split(\\'/\\')[-1]\\n!mkdir -p {local_code_dir}\\n\\n# Write your model.py file here\\n```\\n\\n5. **Deploy the model on SageMaker**\\n```python\\nfrom sagemaker.model import Model\\nfrom sagemaker import get_execution_role\\n\\nrole = get_execution_role()\\nmodel = Model(\\n    image_uri=inference_image_uri,\\n    model_data=f\"s3://{bucket}/{s3_model_prefix}\",\\n    role=role,\\n    source_dir=local_code_dir,\\n)\\n\\ninstance_type = \"ml.g5.24xlarge\"\\ninstance_count = 1\\nmodel_name = f\"{model_name}-sagemaker\"\\nendpoint_name = model_name\\n\\nmodel.deploy(\\n    initial_instance_count=instance_count,\\n    instance_type=instance_type,\\n    endpoint_name=endpoint_name,\\n)\\n```\\n\\nThis will deploy the Baichuan2 model on SageMaker. Note that you may need to adjust the instance type and count based on your requirements and available resources.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('how deploy baichuan2 in sagemaker?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
