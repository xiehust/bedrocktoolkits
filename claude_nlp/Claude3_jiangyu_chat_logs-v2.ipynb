{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 langchain langchain_community -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8db72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646eecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e14612",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_result.csv'\n",
    "# df = pd.read_csv('data_result.csv',nrows=1000)\n",
    "with open(filename) as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e03203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10459762593131|80|2024-03-01 21:46:15,774|{\"uid\":\"\",\"c\":2,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"170930077510459762593131\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"现在都没空\",\"group\":\"\"}\\n',\n",
       " '11065658445287|38|2024-03-01 21:46:17,628|{\"uid\":\"\",\"c\":2,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"170930077711065658445287\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"本盟的。。。\",\"group\":\"\"}\\n',\n",
       " '587495548894|100|2024-03-01 21:46:14,857|{\"uid\":\"\",\"c\":5,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"1709300774587495548894\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"{\\\\\"t\\\\\":2,\\\\\"iconIDs\\\\\":\\\\\"10^10\\\\\",\\\\\"iconID\\\\\":\\\\\"10\\\\\",\\\\\"uts\\\\\":1}\",\"m\":\"link10^10\",\"group\":\"102_5_a7414461f5fb4d73bf0f3f0c6a95686f\"}\\n',\n",
       " '10454393032042|80|2024-03-01 21:46:19,313|{\"uid\":\"\",\"c\":2,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":7,\\\\\"clientTime\\\\\":\\\\\"170930077910454393032042\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"我都兵都上不满\",\"group\":\"\"}\\n',\n",
       " '11055900691941|41|2024-03-01 21:46:20,467|{\"uid\":\"102_1_3557\",\"c\":0,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"170930078011055900691941\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"{\\\\\"t\\\\\":2,\\\\\"iconIDs\\\\\":\\\\\"14^14\\\\\",\\\\\"iconID\\\\\":\\\\\"14\\\\\",\\\\\"uts\\\\\":1}\",\"m\":\"link14^14\",\"group\":\"\"}\\n',\n",
       " '11050545417701|35|2024-03-01 21:46:21,027|{\"uid\":\"\",\"c\":0,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"170930078111050545417701\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"海陆空哪个好\",\"group\":\"\"}\\n',\n",
       " '824408705225|100|2024-03-01 21:46:20,601|{\"uid\":\"\",\"c\":0,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":4,\\\\\"clientTime\\\\\":\\\\\"1709300780824408705225\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"还有50\",\"group\":\"\"}\\n',\n",
       " '8110783499577|80|2024-03-01 21:46:18,284|{\"uid\":\"\",\"c\":0,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"17093007788110783499577\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"あ、少し白髪が出てきててショック受けてます\",\"group\":\"\"}\\n',\n",
       " '8109381586233|75|2024-03-01 21:46:20,050|{\"uid\":\"8100726836537\",\"c\":1,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"17093007798109381586233\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"ないしょか〜(´・ω・｀)\",\"group\":\"\"}\\n',\n",
       " '9213390949450|100|2024-03-01 21:46:19,451|{\"uid\":\"\",\"c\":2,\"custom\":\"{\\\\\"mod\\\\\":\\\\\"\\\\\",\\\\\"modPic\\\\\":0,\\\\\"officialTitle\\\\\":-1,\\\\\"clientTime\\\\\":\\\\\"17093007799213390949450\\\\\",\\\\\"responseData\\\\\":null,\\\\\"atData\\\\\":null,\\\\\"modid\\\\\":0}\",\"llm\":\"\",\"m\":\"算了 我啥都没有\",\"group\":\"\"}\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置要选取的连续元素的长度\n",
    "n = 10\n",
    "\n",
    "# 随机选取一个起始索引\n",
    "start_idx = random.randint(0, len(lines) - n)\n",
    "lines[start_idx:start_idx+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1702e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "extracted_data = []\n",
    "\n",
    "# 提取聊天的正则表达式模式\n",
    "pattern = r'\"m\":\"(.*?)\"'\n",
    "\n",
    "# 过滤出link\n",
    "\n",
    "# 遍历每一行\n",
    "for line in lines:\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        text = match.group(1)\n",
    "        if not text.startswith('link'):\n",
    "            extracted_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cc5947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3921373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac127acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 随机选取一个起始索引\n",
    "def random_cut_arr(arr,n,seed=1):\n",
    "    random.seed(seed)\n",
    "    start_idx = random.randint(0, len(arr) - n)\n",
    "    return arr[start_idx:start_idx+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1200d3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['二大爺好！',\n",
       " 'قول',\n",
       " 'morning All 👋',\n",
       " \"I'm starting to understand\",\n",
       " 'Die spinnen doch',\n",
       " '😂😂😂😂😂😂😂😂',\n",
       " '去群里问问咯',\n",
       " 'not towards our server',\n",
       " '十二点睡 三点醒?',\n",
       " '2221的傻狍子，快打宝藏，我劫一车',\n",
       " '继续？',\n",
       " 'it is heeeeeee',\n",
       " '184 aint doing shıt?',\n",
       " '继续',\n",
       " '네~^^',\n",
       " 'Ooooo fancy 😆',\n",
       " '话',\n",
       " 'tomorrow Baking Master begins, i wouldnt advise spending vit today',\n",
       " 'Once again it shows that Koreans are bullies and bad people',\n",
       " 'Say it ☺️',\n",
       " 'Just the tech tree speed ups',\n",
       " 'ok....lol',\n",
       " 'Ist alles gut! Kannst es ihm sagen. Bin immer bereit um Token zu tauschen. 🤣🤣',\n",
       " 'But you are going to make a much juicier target with them sitting in there',\n",
       " '再来',\n",
       " 'Hi @[3sIR] penguins',\n",
       " 'yaar wo on ka main head hy os ko to chhor do Baki SB collect kro',\n",
       " 'не беси девочку)))))))',\n",
       " '管他呢，反正结局他两区互咬起来就是最大的瓜',\n",
       " '看看我一个人，能不能打过这么多人',\n",
       " 'oui je cherche pas à up mais ça ce fait automatiquement. RL ne nous attaquera pas pour SVS ?',\n",
       " 'i really dont need lyb rn anyways',\n",
       " 'اها يعني كذا',\n",
       " 'I need them for a golden tank',\n",
       " 'Do you think it is enough?',\n",
       " 'doğrudur elektrik santrallere saldırdım sadece o yüzden çıkarttı bende saldırmayacam söz beni ittifaka alın',\n",
       " '对你不利的你都忘了',\n",
       " '去体验下不错的',\n",
       " 'Send me medicine plz, I have bitten my tongue 3 times already',\n",
       " '其实跟42打还是用海军好',\n",
       " 'oh yeah?',\n",
       " 'No talking about terror level 4 🤗',\n",
       " '你为什么这么晚不睡@[l00%] 大肥鱼༒³⁰²⁵ 我姐夫呢！',\n",
       " '那就是深圳?',\n",
       " 'انا غلطان اني بشير حاجه هنا',\n",
       " 'to phla ah das',\n",
       " 'да нет смысла у них сразу киты подходят нам сразу хана',\n",
       " 'We can go up now',\n",
       " 'タイフーン上位来ないかな()',\n",
       " '妮儿要专五才能发动磁暴的吗？',\n",
       " 'What else do we need to touch, to get rewards?',\n",
       " 'Welcome @Bertus',\n",
       " 'I sent in discord last messages',\n",
       " 'i swear, this is the snowflake el.',\n",
       " 'Yup the old crew. Way better. Never really liked anyone on this server anyway.',\n",
       " '이다햄님',\n",
       " 'Bonsoir',\n",
       " '바로 위에 불나는거랑 외ㅏㄴ쪽거 두대 해야할듯 ㅎ',\n",
       " 'that is a very respectable win',\n",
       " '100%',\n",
       " '然后买个50钻',\n",
       " 'Hey Medusa😁',\n",
       " '不到6个小时200名涨了5w',\n",
       " '最后一波',\n",
       " '有我能做的么？',\n",
       " 'サドルすっきやねw',\n",
       " '有军工了，可以霍霍了',\n",
       " '来点人',\n",
       " '*hits 😶',\n",
       " 'Salut 🐝🐝',\n",
       " '🔊 can any of you help with Doris ?',\n",
       " 'guess what? he replied to me after 2 days',\n",
       " '药剂不是实时生效的',\n",
       " 'Can hound 😊',\n",
       " 'He’s gone',\n",
       " 'someone is attacking this',\n",
       " 'i know ♡',\n",
       " '一百八九不保险 有人用了秘密武器 立马就被踢出去了',\n",
       " '私密武器？？',\n",
       " 'You are!',\n",
       " '😂',\n",
       " 'next on my list Lancaster skill 5',\n",
       " 'wystarczy',\n",
       " 'you got that feeling 😂😂',\n",
       " '逆転は不可能だよね？',\n",
       " 'Pregunta, aún podríamos perder el tercer lugar?',\n",
       " 'sorry troc, moved too late',\n",
       " '👍',\n",
       " 'A gdzie 5000',\n",
       " '恭喜夜白成功爆破',\n",
       " 'das ist nicht Marvel das ist 80er',\n",
       " 'Caneminnnnnnn🤭😍😍😍😍',\n",
       " 'そこにこの借金問題やしねー',\n",
       " '你们一般都抽啥烟呐',\n",
       " '有剩的去这个',\n",
       " 'I played it 3 years ago',\n",
       " \"Eventually.. Probably.. I'm not trying to think that far right now lol. I was reffering to that very part when I'd let you feel her. In that very moment I'm definitely grabbing hair and your face is\",\n",
       " 'Oh I have known where she it',\n",
       " '没下田就飞到3星扎堆的地方',\n",
       " '嗯']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cut_arr(extracted_data,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37876e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_extracted_data = random_cut_arr(extracted_data,500)\n",
    "text = \"\\n\".join(cut_extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11947a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e368c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_sonnet = BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                  model_kwargs={\"temperature\": 0,\n",
    "                                \"top_k\":10,\n",
    "                                \"max_tokens\": 1024,\n",
    "                                \"top_p\":0.5,\n",
    "                                # \"stop_sequences\":['</response>']\n",
    "                               },\n",
    "                  streaming=True,callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm_haiku = BedrockChat(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                  model_kwargs={\"temperature\": 0,\n",
    "                                \"top_k\":10,\n",
    "                                \"max_tokens\": 1024,\n",
    "                                \"top_p\":0.5,\n",
    "                                # \"stop_sequences\":['</response>']\n",
    "                               },\n",
    "                  streaming=True,callbacks=[StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_extract = \\\n",
    "\"\"\"You are an expert research assistant, tasked with identifying player sentiments regarding certain in-game items, neutral NPCs, and game market activities. \n",
    "\n",
    "Here is a document you will analyze\n",
    "<doc>\n",
    "{context}\n",
    "</doc>\n",
    "\n",
    "Here is a task:\n",
    "First, find the quotes from the document that are most relevant to {topic}, and then print them in numbered order. Quotes should be relatively short.\n",
    "If there are no relevant quotes, write \"No relevant quotes\" instead.\n",
    "please enclose your analysis results in xml tag <response>.\n",
    "\n",
    "for example:\n",
    "<response>\n",
    "1. \"拍卖行多香\" \n",
    "2. \"我拍到好东西了\" \n",
    "3. \"拍卖行太差劲了\" \n",
    "4. \"auction sucks\" \n",
    "5. \"拍卖行有人发包\" \n",
    "</response>\n",
    "\n",
    "Skip the preamble, go straight into the answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76207d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_sentiment = \\\n",
    "\"\"\"You are a chat message sentiment classifer\n",
    "\n",
    "Here is a document you will classify the senetiment\n",
    "<doc>\n",
    "{relevant_info}\n",
    "</doc>\n",
    "\n",
    "\n",
    "please list all the content if it is relevant to {topic} and classify the sentiment of each content into [positive,neutral,negative]'\n",
    "\n",
    "Please follow below requirements:\n",
    "1. You will strictly be based on the document in <doc>.\n",
    "2. please enclose your analysis results in xml tag <sentiment>.\n",
    "\n",
    "for example:\n",
    "<sentiment>\n",
    "1. \"拍卖行多香\" [positive]\n",
    "2. \"我拍到好东西了\" [positive]\n",
    "3. \"拍卖行太差劲了\" [negative]\n",
    "4. \"auction sucks\" [negative]\n",
    "5. \"拍卖行有人发包\" [neutral]\n",
    "</sentiment>\n",
    "\n",
    "Skip the preamble, go straight into the answer. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add1efa-e01c-45da-8949-b85eee521677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "import re\n",
    "class CustOuputParser(BaseOutputParser[str]):\n",
    "\n",
    "    def extract(self,content:str) -> str:\n",
    "        pattern = r\"<response>(.*?)</response>\"\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        if match:\n",
    "            text = match.group(1)\n",
    "            text = text.replace('[','(').replace(']',')') ##避免跟sentiment的格式冲突\n",
    "            return text\n",
    "        else:\n",
    "            return 'No relevant quotes'    \n",
    "    \n",
    "    def parse(self, text: str) -> str:\n",
    "        cleaned_text = self.extract(text)\n",
    "        return cleaned_text\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"cust_output_parser\"\n",
    "\n",
    "\n",
    "class CustOuputParser2(BaseOutputParser[str]):\n",
    "\n",
    "    def extract(self,content:str) -> str:\n",
    "        pattern = r\"<sentiment>(.*?)</sentiment>\"\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        return match.group(1) if match else 'No sentiment'    \n",
    "    \n",
    "    def parse(self, text: str) -> str:\n",
    "        cleaned_text = self.extract(text)\n",
    "        return cleaned_text\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"cust_output_parser\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71fb61-a093-4c79-8ae0-280d6e35b910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(arr, n):\n",
    "    chunks = []\n",
    "    for i in range(0, len(arr), n):\n",
    "        chunks.append(arr[i:i+n])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82656a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(extracted_data,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0403acb-c13a-412d-af51-aac756552703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total chunks {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22dbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"\\n\".join(chunks[45])\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt_extract = ChatPromptTemplate.from_template(template_extract)\n",
    "prompt_sentiment = ChatPromptTemplate.from_template(template_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f144e41-6388-47d5-85ce-76c25c2f6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "## 如果没有相关内容则无须invoke chain2\n",
    "def route(info):\n",
    "    if not 'no relevant quotes' in info['relevant_info'].lower():\n",
    "        return chain_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc312ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustOuputParser()\n",
    "output2_parser  = CustOuputParser2()\n",
    "\n",
    "chain_1 = prompt_extract | llm_sonnet |output_parser\n",
    "chain_2 = prompt_sentiment | llm_sonnet|output2_parser\n",
    "# chain_3 = ({'relevant_info':chain_1,'topic':itemgetter('topic')})|prompt_sentiment | llm_haiku|output2_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e5a17-fecc-40ea-a35a-c26689fe6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试chain3\n",
    "# chain_3.invoke({'topic':\"auction house\",\n",
    "#                        'context':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef25c2-9076-43e5-8c7c-3f072dc54a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试加入了路由选择的chainfull\n",
    "full_chain = ({'relevant_info':chain_1,'topic':itemgetter('topic')})| RunnableLambda(route)\n",
    "answer = full_chain.invoke({'topic':\"auction house\",'context':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b0c47-2849-485e-88ef-5aeb45eeb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58321e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"is there any content relevant to auction house?\"\n",
    "\n",
    "#'please list all the content if it is relevant and classify the sentiment of each content into [positive,neutral,negative]'\n",
    "\n",
    "# answer = chain_2.invoke({'topic':\"auction house\",\n",
    "#                        'context':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087d996-eced-4012-9f71-2706b56f29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_sentiment(text) -> list:\n",
    "    pattern = r'^(.*?)\\s*\\[(.*?)\\]'\n",
    "    # pattern = r'\"(.*?)\"\\s*\\[(.*?)\\]$'\n",
    "    if not text:\n",
    "        return []\n",
    "    result = []\n",
    "    for line in text.split('\\n'):\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            # print(match)\n",
    "            text_part = match.group(1)\n",
    "            quoted_texts = re.findall(r'\"(.*?)\"', text_part)\n",
    "            # print(quoted_texts)\n",
    "            sentiment_part = match.group(2)\n",
    "            result.append((quoted_texts[0], sentiment_part))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca16752",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_and_sentiment(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a list to a local file, if the file is exist then append the list to the file\n",
    "def save_list(list,file_name):\n",
    "    with open(file_name,'a') as f:\n",
    "        for item in list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    f.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a list to a csv file using pandas, if the file is exist then append the list to the file\n",
    "import pandas as pd\n",
    "import os\n",
    "def save_to_csv(data, filename):\n",
    "    new_data = pd.DataFrame(data, columns=['text','sentiment'])\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        #append data to df\n",
    "        df = pd.concat([df, new_data], ignore_index=True)\n",
    "    else:\n",
    "        df = new_data\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7d833",
   "metadata": {},
   "source": [
    "循环每个chunk进行输出,每次追加到csv文件保存，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "all_result = []\n",
    "filename = 'parsed_result'\n",
    "\n",
    "##断点跳过\n",
    "skip_num =238\n",
    "for i,chunk in enumerate(chunks[:1000]):\n",
    "    if i < skip_num:\n",
    "        continue\n",
    "    t1 = time.time()\n",
    "    print(f'--------chunk idx:{i}-------')\n",
    "    text =  \"\\n\".join(chunk)\n",
    "    answer = full_chain.invoke({'topic':\"auction house\",\n",
    "                       'context':text})\n",
    "    extract_ret = extract_text_and_sentiment(answer)\n",
    "    print(f'\\nextract_ret:\\n{extract_ret}')\n",
    "    time.sleep(2)\n",
    "    print(f'---time cost {time.time()-t1} s -----\\nsleep 10s for token tpm\\n\\n')\n",
    "    if extract_ret:\n",
    "        all_result+=extract_ret\n",
    "        save_to_csv(extract_ret,filename+f\"chunk_{i}.csv\")\n",
    "        save_to_csv(extract_ret,filename+f\"_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f31179",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a5c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
