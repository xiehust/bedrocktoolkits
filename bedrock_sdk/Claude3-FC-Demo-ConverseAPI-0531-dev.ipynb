{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5f51704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.35.40)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.35.46-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.0)\n",
      "Collecting botocore<1.36.0,>=1.35.46 (from boto3)\n",
      "  Downloading botocore-1.35.46-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.46->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.46->boto3) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.46->boto3) (1.16.0)\n",
      "Downloading boto3-1.35.46-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.35.46-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.40\n",
      "    Uninstalling botocore-1.35.40:\n",
      "      Successfully uninstalled botocore-1.35.40\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.40\n",
      "    Uninstalling boto3-1.35.40:\n",
      "      Successfully uninstalled boto3-1.35.40\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scoutsuite 5.14.0 requires python-dateutil<2.8.1,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.35.46 botocore-1.35.46\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 termcolor -U\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ef48e",
   "metadata": {},
   "source": [
    "## Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44403c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StationNotFoundError(Exception):\n",
    "    \"\"\"Raised when a radio station isn't found.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_top_song(sign):\n",
    "    \"\"\"Returns the most popular song for the requested station.\n",
    "    Args:\n",
    "        call_sign (str): The call sign for the station for which you want\n",
    "        the most popular song.\n",
    "\n",
    "    Returns:\n",
    "        response (json): The most popular song and artist.\n",
    "    \"\"\"\n",
    "\n",
    "    song = \"\"\n",
    "    artist = \"\"\n",
    "    if sign == 'WZPZ':\n",
    "        song = \"Elemental Hotel\"\n",
    "        artist = \"8 Storey Hike\"\n",
    "    elif sign == 'WZHK':\n",
    "        song = \"Elemental Hotel\"\n",
    "        artist = \"8 Storey Hike\"\n",
    "    else:\n",
    "        raise StationNotFoundError(f\"Station {sign} not found.\")\n",
    "\n",
    "    return {\"json\": {\"song\": song, \"artist\": artist}}\n",
    "\n",
    "tools_imp = {\n",
    "    'top_song': get_top_song,\n",
    "}\n",
    "\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"top_song\",\n",
    "                \"description\": \"Get the most popular song played on a radio station.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sign\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The call sign for the radio station for which you want the most popular song. Example calls signs are WZPZ, and WKRP.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"sign\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def invoke_tool(tool):\n",
    "    print('\\nInvoking tool: %s. params: %s. id: %s' % (tool['name'], tool['input'], tool['toolUseId']))\n",
    "    if tool['name'] in tools_imp:\n",
    "        tool_result = tools_imp[tool['name']](**tool['input'])\n",
    "        print(\"Tool invoke id: %s, result: %s\" % (tool['toolUseId'], tool_result))\n",
    "        return tool_result\n",
    "    else:\n",
    "        print(f\"Tool {tool['name']} not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcde72a",
   "metadata": {},
   "source": [
    "## 流式模式使用 Tool use 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcb64ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shows how to use the Conversation API to stream a response from Anthropic Claude 3 (on demand).\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def print_metadata(metadata):\n",
    "    \"\"\"\n",
    "    print metadata of streaming response: such as in, out and total tokens, latency(if has)\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if 'usage' in metadata:\n",
    "        print(\"\\nToken usage\")\n",
    "        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "        print(\n",
    "            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "    if 'metrics' in metadata:\n",
    "        print(\n",
    "            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "\n",
    "def stream_conversation(bedrock_client,\n",
    "                    model_id,\n",
    "                    messages,\n",
    "                    system_prompts,\n",
    "                    inference_config,\n",
    "                    additional_model_fields,\n",
    "                    tool_config,\n",
    "                    round=0):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'\\n************ ROUND {round} START ************')\n",
    "    print(\"Streaming messages with model %s\" % model_id)\n",
    "\n",
    "    bedrock_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config,\n",
    "        \"additionalModelRequestFields\": additional_model_fields,\n",
    "        \"toolConfig\": tool_config\n",
    "    }\n",
    "\n",
    "    system = [item for item in system_prompts if item.get('text')]\n",
    "    if system:\n",
    "        bedrock_params['system'] = system\n",
    "\n",
    "    response = bedrock_client.converse_stream( **bedrock_params )\n",
    "    stream = response.get('stream')\n",
    "    tools_buf = {}\n",
    "    resp_text_buf = ''\n",
    "    if stream:\n",
    "        stop_reason = None\n",
    "        metadata = None\n",
    "        for event in stream:\n",
    "            # print(event)\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockStart' in event:\n",
    "                if 'toolUse' in event['contentBlockStart']['start']:\n",
    "                    block_index = event['contentBlockStart']['contentBlockIndex']\n",
    "                    tools_buf[block_index] = event['contentBlockStart']['start']['toolUse']\n",
    "                    tools_buf[block_index]['input'] = ''\n",
    "            if 'contentBlockDelta' in event:\n",
    "                block_index = event['contentBlockDelta']['contentBlockIndex']\n",
    "                delta_types = event['contentBlockDelta']['delta'].keys()\n",
    "                if 'text' in delta_types:\n",
    "                    text_delta = event['contentBlockDelta']['delta']['text']\n",
    "                    print(colored(text_delta, 'green'), end=\"\")\n",
    "                    resp_text_buf += text_delta\n",
    "                elif 'toolUse' in delta_types:\n",
    "                    tools_buf[block_index]['input'] += event['contentBlockDelta']['delta']['toolUse']['input']\n",
    "                    # print(event['contentBlockDelta']['delta']['toolUse']['input'], end=\"\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                stop_reason = event['messageStop']['stopReason']\n",
    "                # print(f\"\\nStop reason: { stop_reason }\")\n",
    "\n",
    "        # put assistant response into messages\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": []\n",
    "        }\n",
    "        if resp_text_buf:\n",
    "            assistant_message['content'].append({ \"text\": resp_text_buf })\n",
    "\n",
    "        messages.append(assistant_message)\n",
    "\n",
    "        # parse tool use response, invoke tools and trigger next inference automatically\n",
    "        if 'tool_use' == stop_reason:\n",
    "            # store tool use result\n",
    "            tool_result_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": []\n",
    "            }\n",
    "\n",
    "            for tool in tools_buf.values():\n",
    "                tool['input'] = json.loads(tool['input'])\n",
    "                assistant_message['content'].append( {'toolUse': tool } )\n",
    "\n",
    "                tool_result = {}\n",
    "                try:\n",
    "                    tool_result = {\n",
    "                        \"toolUseId\": tool['toolUseId'],\n",
    "                        \"content\": [ invoke_tool(tool) ]\n",
    "                    }\n",
    "                except StationNotFoundError as err:\n",
    "                    tool_result = {\n",
    "                        \"toolUseId\": tool['toolUseId'],\n",
    "                        \"content\": [{\"text\":  err.args[0]}],\n",
    "                        \"status\": 'error'\n",
    "                    }\n",
    "                tool_result_message['content'].append({\"toolResult\": tool_result})\n",
    "            \n",
    "            # put tool invocation result into messages as user message\n",
    "            messages.append(tool_result_message)\n",
    "            print_metadata(metadata)\n",
    "            # trigger next inference\n",
    "            stream_conversation(bedrock_client, model_id, messages, system_prompts, \n",
    "                                inference_config, additional_model_fields, tool_config, round + 1)\n",
    "        else:\n",
    "            print_metadata(metadata)\n",
    "        print(f\"----tools_buf----:\\n{tools_buf}\")\n",
    "        return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e341081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mQuestion: What is the most popular song on WZPZ and WZHK?\u001b[0m\n",
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model anthropic.claude-3-5-sonnet-20241022-v2:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:A client error occurred: The maximum tokens you requested exceeds the model limit of 4096. Try again with a maximum tokens value that is lower than 4096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: The maximum tokens you requested exceeds the model limit of 4096. Try again with a maximum tokens value that is lower than 4096.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for streaming message API response example.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    # model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    # model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    # model_id = \"meta.llama3-1-70b-instruct-v1:0\" \n",
    "    # model_id = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "    # model_id = \"cohere.command-r-v1:0\" # doesn't support tool use in streaming mode\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "keke    system_prompt = \"\"\n",
    "\n",
    "    # Message to send to the model.\n",
    "    input_text = \"What is the most popular song on WZPZ and WZHK?\"\n",
    "    print(colored(f\"Question: {input_text}\", 'red'))\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    \n",
    "    # System prompts.\n",
    "    system_prompts = [{\"text\" : system_prompt}]\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.9\n",
    "    top_k = 200\n",
    "    max_tokens = 8000\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    try:\n",
    "        session = boto3.session.Session(profile_name='default',region_name='us-west-2')\n",
    "        bedrock_client = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "        stream_conversation(bedrock_client, model_id, messages,\n",
    "                        system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "              format(message))\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nFinished streaming messages with model {model_id}.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2173bd93",
   "metadata": {},
   "source": [
    "## 非流式模式使用 Tool use 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac87298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to use tools with the Conversation API and the Claude model.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_text(bedrock_client, \n",
    "                  model_id, \n",
    "                  messages, \n",
    "                  system_prompts, \n",
    "                  inference_config, \n",
    "                  additional_model_fields, \n",
    "                  tool_config, \n",
    "                  round=0):\n",
    "    \"\"\"Generates text using the supplied Amazon Bedrock model. If necessary,\n",
    "    the function handles tool use requests and sends the result to the model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The Amazon Bedrock model ID.\n",
    "        tool_config (dict): The tool configuration.\n",
    "        messages (list): The input messages.\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating text with model %s\", model_id)\n",
    "\n",
    "\n",
    "    bedrock_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config,\n",
    "        \"additionalModelRequestFields\": additional_model_fields,\n",
    "        \"toolConfig\": tool_config\n",
    "    }\n",
    "\n",
    "    system = [item for item in system_prompts if item.get('text')]\n",
    "    if system:\n",
    "        bedrock_params['system'] = system\n",
    "\n",
    "\n",
    "    response = bedrock_client.converse(**bedrock_params)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "    output_content = output_message['content']\n",
    "\n",
    "    tool_list = []\n",
    "    tool_result_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": []\n",
    "    }\n",
    "\n",
    "    for item in output_content:\n",
    "        if 'text' in item:\n",
    "            print(colored(item['text'], 'green'))\n",
    "\n",
    "        if 'toolUse' in item:\n",
    "            tool_list.append(item['toolUse'])\n",
    "\n",
    "    for tool in tool_list:\n",
    "        logger.info(\"Requesting tool %s. Request: %s\",\n",
    "                        tool['name'], tool['toolUseId'])\n",
    "\n",
    "        tool_result = {}\n",
    "        try:\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool['toolUseId'],\n",
    "                \"content\": [ invoke_tool(tool) ]\n",
    "            }\n",
    "        except StationNotFoundError as err:\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool['toolUseId'],\n",
    "                \"content\": [{\"text\":  err.args[0]}],\n",
    "                \"status\": 'error'\n",
    "            }\n",
    "\n",
    "        tool_result_message['content'].append({\n",
    "            \"toolResult\": tool_result\n",
    "        })\n",
    "    if tool_list:\n",
    "        messages.append(tool_result_message)\n",
    "        generate_text(bedrock_client, model_id, messages, system_prompts, \n",
    "                      inference_config, additional_model_fields, tool_config, round + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ddb8aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mQuestion: What is the most popular song on WZPZ and WZHK? think step by step\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 48\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished generating text with model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mgenerate_text\u001b[49m(bedrock_client, model_id, messages,\n\u001b[1;32m     35\u001b[0m                     system_prompts, inference_config, additional_model_fields, tool_config)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     38\u001b[0m     message \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for tool use example.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.9\n",
    "    top_k = 200\n",
    "    max_tokens = 2000\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    system_prompts = []\n",
    "    input_text = \"What is the most popular song on WZPZ and WZHK? think step by step\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }]\n",
    "    session = boto3.session.Session(profile_name='c35',region_name='default')\n",
    "    bedrock_client = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "    try:\n",
    "        print(colored(f\"Question: {input_text}\", 'red'))\n",
    "        generate_text(bedrock_client, model_id, messages,\n",
    "                        system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(f\"A client error occured: {message}\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text with model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93302f71",
   "metadata": {},
   "source": [
    "## 多轮对话示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716b5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "config = Config(read_timeout=1000) # second\n",
    "\n",
    "# model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# model_id = \"anthropic.claude-3-opus-20240229-v1:0\" \n",
    "model_id = \"cohere.command-r-v1:0\" # doesn't support tool use in streaming mode\n",
    "\n",
    "# inference parameters to use.\n",
    "temperature = 0.9\n",
    "top_k = 200\n",
    "max_tokens = 2000\n",
    "# Base inference parameters.\n",
    "inference_config = {\n",
    "    \"temperature\": temperature,\n",
    "    \"maxTokens\": max_tokens,\n",
    "}\n",
    "\n",
    "if model_id.startswith('anthropic.'):\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "else:\n",
    "    additional_model_fields = {}\n",
    "\n",
    "class HM:\n",
    "    def __init__(self, system_prompt=[]):\n",
    "        self.msgs = []\n",
    "        self.max_retries = 1\n",
    "        self.current_num_retries = 0\n",
    "        self.bedrock_client = boto3.client(service_name='bedrock-runtime', config=config)\n",
    "        self.system_prompts = [{\"text\": sp for sp in system_prompt if sp}]\n",
    "\n",
    "    def _put_user(self, user):\n",
    "        if user:\n",
    "            self.msgs.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user}]\n",
    "            })\n",
    "\n",
    "    def _put_assist(self, assist):\n",
    "        if assist:\n",
    "            self.msgs.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"text\": assist}]\n",
    "            })\n",
    "\n",
    "    def chat(self, user, assistant=None, stream=True):\n",
    "        self._put_user(user)\n",
    "        self._put_assist(assistant)\n",
    "\n",
    "        try:\n",
    "            if stream:\n",
    "                stream_conversation(self.bedrock_client, model_id, self.msgs,\n",
    "                            self.system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "            else:\n",
    "                generate_text(self.bedrock_client, model_id, self.msgs,\n",
    "                            self.system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "        except ClientError as err:\n",
    "            message = err.response['Error']['Message']\n",
    "            logger.error(\"A client error occurred: %s\", message)\n",
    "            print(\"A client error occured: \" +\n",
    "                format(message))\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\nFinished streaming messages with model {model_id}.\")\n",
    "\n",
    "    def print_msg(self):\n",
    "        print(json.dumps(self.msgs, ensure_ascii=False, indent=2))\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.msgs.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51c7fc",
   "metadata": {},
   "source": [
    "### Claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f642ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:A client error occurred: This model doesn't support tool use in streaming mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: This model doesn't support tool use in streaming mode.\n",
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:A client error occurred: This model doesn't support tool use in streaming mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: This model doesn't support tool use in streaming mode.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the most popular song on WZPZ and WZHK?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"北京天气如何?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "hm_1 = HM()\n",
    "hm_1.chat(\"What is the most popular song on WZPZ and WZHK?\")\n",
    "hm_1.chat(\"北京天气如何?\")\n",
    "hm_1.print_msg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4558f33",
   "metadata": {},
   "source": [
    "### Cohere Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997a8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n",
      "INFO:__main__:Requesting tool top_song. Request: tooluse_PP11vdEPSjKewrFPt2WGig\n",
      "INFO:__main__:Requesting tool top_song. Request: tooluse_zKBi1vNWTPmmOL_PBl325w\n",
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI will run concurrent searches for each radio station to find their most popular songs, and relay this information to the user.\u001b[0m\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZPZ'}. id: tooluse_PP11vdEPSjKewrFPt2WGig\n",
      "Tool invoke id: tooluse_PP11vdEPSjKewrFPt2WGig, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZHK'}. id: tooluse_zKBi1vNWTPmmOL_PBl325w\n",
      "Tool invoke id: tooluse_zKBi1vNWTPmmOL_PBl325w, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThe most popular song on both WZPZ and WZHK is Elemental Hotel by 8 Storey Hike.\u001b[0m\n",
      "\n",
      "Finished streaming messages with model cohere.command-r-v1:0.\n",
      "\u001b[32mI'm afraid I can't find any information about the weather in Beijing. Sorry about that!\u001b[0m\n",
      "\n",
      "Finished streaming messages with model cohere.command-r-v1:0.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the most popular song on WZPZ and WZHK?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"I will run concurrent searches for each radio station to find their most popular songs, and relay this information to the user.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_PP11vdEPSjKewrFPt2WGig\",\n",
      "          \"name\": \"top_song\",\n",
      "          \"input\": {\n",
      "            \"sign\": \"WZPZ\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_zKBi1vNWTPmmOL_PBl325w\",\n",
      "          \"name\": \"top_song\",\n",
      "          \"input\": {\n",
      "            \"sign\": \"WZHK\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_PP11vdEPSjKewrFPt2WGig\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"json\": {\n",
      "                \"song\": \"Elemental Hotel\",\n",
      "                \"artist\": \"8 Storey Hike\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_zKBi1vNWTPmmOL_PBl325w\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"json\": {\n",
      "                \"song\": \"Elemental Hotel\",\n",
      "                \"artist\": \"8 Storey Hike\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The most popular song on both WZPZ and WZHK is Elemental Hotel by 8 Storey Hike.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"北京天气如何?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"I'm afraid I can't find any information about the weather in Beijing. Sorry about that!\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "hm_2 = HM()\n",
    "hm_2.chat(\"What is the most popular song on WZPZ and WZHK?\", None, False)\n",
    "hm_2.chat(\"北京天气如何?\", None, False)\n",
    "hm_2.print_msg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdc9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Can't invoke 'meta.llama3-2-3b-instruct-v1:0'. Reason: An error occurred (ValidationException) when calling the ConverseStream operation: The provided model doesn't support on-demand throughput.\n"
     ]
    }
   ],
   "source": [
    "# Use the Conversation API to send a text message to Anthropic Claude\n",
    "# and print the response stream.\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "model_id = \"meta.llama3-2-3b-instruct-v1:0\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    streaming_response = client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the streamed response text in real-time.\n",
    "    for chunk in streaming_response[\"stream\"]:\n",
    "        if \"contentBlockDelta\" in chunk:\n",
    "            text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "            print(text, end=\"\")\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87b9f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of a 'hello world' program is to serve as a simple and straightforward introduction to programming by displaying the message \"Hello, World!\" as the output."
     ]
    }
   ],
   "source": [
    "# Use the Conversation API to send a text message to Anthropic Claude\n",
    "# and print the response stream.\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.config import Config\n",
    "\n",
    "my_config = Config(\n",
    "    connect_timeout=10,  # Connection timeout in seconds\n",
    "    read_timeout=120,   # Read timeout in seconds (adjust as needed)\n",
    "    retries={'max_attempts': 3}  # Optional: Set max retry attempts\n",
    ")\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\",config=my_config)\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# model_id = \"meta.llama3-2-3b-instruct-v1:0\"\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    streaming_response = client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the streamed response text in real-time.\n",
    "    for chunk in streaming_response[\"stream\"]:\n",
    "        if \"contentBlockDelta\" in chunk:\n",
    "            text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "            print(text, end=\"\")\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5709d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
