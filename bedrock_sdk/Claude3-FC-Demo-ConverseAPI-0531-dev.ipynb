{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f51704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install boto3 termcolor -U\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ef48e",
   "metadata": {},
   "source": [
    "## Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44403c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StationNotFoundError(Exception):\n",
    "    \"\"\"Raised when a radio station isn't found.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_top_song(sign):\n",
    "    \"\"\"Returns the most popular song for the requested station.\n",
    "    Args:\n",
    "        call_sign (str): The call sign for the station for which you want\n",
    "        the most popular song.\n",
    "\n",
    "    Returns:\n",
    "        response (json): The most popular song and artist.\n",
    "    \"\"\"\n",
    "\n",
    "    song = \"\"\n",
    "    artist = \"\"\n",
    "    if sign == 'WZPZ':\n",
    "        song = \"Elemental Hotel\"\n",
    "        artist = \"8 Storey Hike\"\n",
    "    elif sign == 'WZHK':\n",
    "        song = \"Elemental Hotel\"\n",
    "        artist = \"8 Storey Hike\"\n",
    "    else:\n",
    "        raise StationNotFoundError(f\"Station {sign} not found.\")\n",
    "\n",
    "    return {\"json\": {\"song\": song, \"artist\": artist}}\n",
    "\n",
    "tools_imp = {\n",
    "    'top_song': get_top_song,\n",
    "}\n",
    "\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"top_song\",\n",
    "                \"description\": \"Get the most popular song played on a radio station.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sign\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The call sign for the radio station for which you want the most popular song. Example calls signs are WZPZ, and WKRP.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"sign\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def invoke_tool(tool):\n",
    "    print('\\nInvoking tool: %s. params: %s. id: %s' % (tool['name'], tool['input'], tool['toolUseId']))\n",
    "    if tool['name'] in tools_imp:\n",
    "        tool_result = tools_imp[tool['name']](**tool['input'])\n",
    "        print(\"Tool invoke id: %s, result: %s\" % (tool['toolUseId'], tool_result))\n",
    "        return tool_result\n",
    "    else:\n",
    "        print(f\"Tool {tool['name']} not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcde72a",
   "metadata": {},
   "source": [
    "## 流式模式使用 Tool use 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb64ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2067242721.py, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 140\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"content\": [{\"text\":  err.args[0]}],\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Shows how to use the Conversation API to stream a response from Anthropic Claude 3 (on demand).\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def print_metadata(metadata):\n",
    "    \"\"\"\n",
    "    print metadata of streaming response: such as in, out and total tokens, latency(if has)\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if 'usage' in metadata:\n",
    "        print(\"\\nToken usage\")\n",
    "        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "        print(\n",
    "            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "    if 'metrics' in metadata:\n",
    "        print(\n",
    "            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "\n",
    "def stream_conversation(bedrock_client,\n",
    "                    model_id,\n",
    "                    messages,\n",
    "                    system_prompts,\n",
    "                    inference_config,\n",
    "                    additional_model_fields,\n",
    "                    tool_config,\n",
    "                    round=0):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'\\n************ ROUND {round} START ************')\n",
    "    print(\"Streaming messages with model %s\" % model_id)\n",
    "\n",
    "    bedrock_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config,\n",
    "        \"additionalModelRequestFields\": additional_model_fields,\n",
    "        \"toolConfig\": tool_config\n",
    "    }\n",
    "\n",
    "    system = [item for item in system_prompts if item.get('text')]\n",
    "    if system:\n",
    "        bedrock_params['system'] = system\n",
    "\n",
    "    response = bedrock_client.converse_stream( **bedrock_params )\n",
    "    stream = response.get('stream')\n",
    "    tools_buf = {}\n",
    "    resp_text_buf = ''\n",
    "    if stream:\n",
    "        stop_reason = None\n",
    "        metadata = None\n",
    "        for event in stream:\n",
    "            # print(event)\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockStart' in event:\n",
    "                if 'toolUse' in event['contentBlockStart']['start']:\n",
    "                    block_index = event['contentBlockStart']['contentBlockIndex']\n",
    "                    tools_buf[block_index] = event['contentBlockStart']['start']['toolUse']\n",
    "                    tools_buf[block_index]['input'] = ''\n",
    "            if 'contentBlockDelta' in event:\n",
    "                block_index = event['contentBlockDelta']['contentBlockIndex']\n",
    "                delta_types = event['contentBlockDelta']['delta'].keys()\n",
    "                if 'text' in delta_types:\n",
    "                    text_delta = event['contentBlockDelta']['delta']['text']\n",
    "                    print(colored(text_delta, 'green'), end=\"\")\n",
    "                    resp_text_buf += text_delta\n",
    "                elif 'toolUse' in delta_types:\n",
    "                    tools_buf[block_index]['input'] += event['contentBlockDelta']['delta']['toolUse']['input']\n",
    "                    # print(event['contentBlockDelta']['delta']['toolUse']['input'], end=\"\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                stop_reason = event['messageStop']['stopReason']\n",
    "                # print(f\"\\nStop reason: { stop_reason }\")\n",
    "\n",
    "        # put assistant response into messages\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": []\n",
    "        }\n",
    "        if resp_text_buf:\n",
    "            assistant_message['content'].append({ \"text\": resp_text_buf })\n",
    "\n",
    "        messages.append(assistant_message)\n",
    "\n",
    "        # parse tool use response, invoke tools and trigger next inference automatically\n",
    "        if 'tool_use' == stop_reason:\n",
    "            # store tool use result\n",
    "            tool_result_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": []\n",
    "            }\n",
    "\n",
    "            for tool in tools_buf.values():\n",
    "                tool['input'] = json.loads(tool['input'])\n",
    "                assistant_message['content'].append( {'toolUse': tool } )\n",
    "\n",
    "                tool_result = {}\n",
    "                try:\n",
    "                    tool_result = {\n",
    "                        \"toolUseId\": tool['toolUseId'],\n",
    "                        \"content\": [ invoke_tool(tool) ]\n",
    "                    }\n",
    "                except StationNotFoundError as err:\n",
    "                    tool_result = {\n",
    "                        \"toolUseId\": tool['toolUseId'],\n",
    "                        \"content\": [{\"text\":  err.args[0]}],\n",
    "                        \"status\": 'error'\n",
    "                    }\n",
    "                tool_result_message['content'].append({\"toolResult\": tool_result})\n",
    "            \n",
    "            # put tool invocation result into messages as user message\n",
    "            messages.append(tool_result_message)\n",
    "            print_metadata(metadata)\n",
    "            # trigger next inference\n",
    "            stream_conversation(bedrock_client, model_id, messages, system_prompts, \n",
    "                                inference_config, additional_model_fields, tool_config, round + 1)\n",
    "        else:\n",
    "            print_metadata(metadata)\n",
    "        print(f\"----tools_buf----:\\n{tools_buf}\")\n",
    "        return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e341081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mQuestion: What is the most popular song on WZPZ and WZHK?\u001b[0m\n",
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model anthropic.claude-3-sonnet-20240229-v1:0\n",
      "\n",
      "Role: assistant\n",
      "\u001b[32mOkay\u001b[0m\u001b[32m,\u001b[0m\u001b[32m let\u001b[0m\u001b[32m's\u001b[0m\u001b[32m fin\u001b[0m\u001b[32md the\u001b[0m\u001b[32m most\u001b[0m\u001b[32m popular\u001b[0m\u001b[32m song\u001b[0m\u001b[32m for\u001b[0m\u001b[32m the\u001b[0m\u001b[32m radio\u001b[0m\u001b[32m stations\u001b[0m\u001b[32m WZ\u001b[0m\u001b[32mP\u001b[0m\u001b[32mZ\u001b[0m\u001b[32m an\u001b[0m\u001b[32md W\u001b[0m\u001b[32mZH\u001b[0m\u001b[32mK\u001b[0m\u001b[32m:\u001b[0m\n",
      "Invoking tool: top_song. params: {'sign': 'WZPZ'}. id: tooluse_ITChwiCDQ-q12g8V3sGC-w\n",
      "Tool invoke id: tooluse_ITChwiCDQ-q12g8V3sGC-w, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n",
      "\n",
      "Token usage\n",
      "Input tokens: 277\n",
      ":Output tokens: 67\n",
      ":Total tokens: 344\n",
      "Latency: 1585 milliseconds\n",
      "\n",
      "************ ROUND 1 START ************\n",
      "Streaming messages with model anthropic.claude-3-sonnet-20240229-v1:0\n",
      "\n",
      "Role: assistant\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZHK'}. id: tooluse_zdRe_qFfRmqiBXI50y5FDQ\n",
      "Tool invoke id: tooluse_zdRe_qFfRmqiBXI50y5FDQ, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n",
      "\n",
      "Token usage\n",
      "Input tokens: 388\n",
      ":Output tokens: 43\n",
      ":Total tokens: 431\n",
      "Latency: 1009 milliseconds\n",
      "\n",
      "************ ROUND 2 START ************\n",
      "Streaming messages with model anthropic.claude-3-sonnet-20240229-v1:0\n",
      "\n",
      "Role: assistant\n",
      "\u001b[32mThe\u001b[0m\u001b[32m most\u001b[0m\u001b[32m popular\u001b[0m\u001b[32m song\u001b[0m\u001b[32m on\u001b[0m\u001b[32m both\u001b[0m\u001b[32m WZ\u001b[0m\u001b[32mP\u001b[0m\u001b[32mZ\u001b[0m\u001b[32m an\u001b[0m\u001b[32md W\u001b[0m\u001b[32mZH\u001b[0m\u001b[32mK\u001b[0m\u001b[32m is\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mEle\u001b[0m\u001b[32mmental\u001b[0m\u001b[32m Hotel\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m by\u001b[0m\u001b[32m the\u001b[0m\u001b[32m artist\u001b[0m\u001b[32m \u001b[0m\u001b[32m8\u001b[0m\u001b[32m \u001b[0m\u001b[32mSto\u001b[0m\u001b[32mrey\u001b[0m\u001b[32m H\u001b[0m\u001b[32mike\u001b[0m\u001b[32m.\u001b[0m\n",
      "Token usage\n",
      "Input tokens: 475\n",
      ":Output tokens: 39\n",
      ":Total tokens: 514\n",
      "Latency: 1328 milliseconds\n",
      "----tools_buf----:\n",
      "{}\n",
      "----tools_buf----:\n",
      "{0: {'toolUseId': 'tooluse_zdRe_qFfRmqiBXI50y5FDQ', 'name': 'top_song', 'input': {'sign': 'WZHK'}}}\n",
      "----tools_buf----:\n",
      "{1: {'toolUseId': 'tooluse_ITChwiCDQ-q12g8V3sGC-w', 'name': 'top_song', 'input': {'sign': 'WZPZ'}}}\n",
      "\n",
      "Finished streaming messages with model anthropic.claude-3-sonnet-20240229-v1:0.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for streaming message API response example.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    # model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    # model_id = \"anthropic.claude-3-opus-20240229-v1:0\" \n",
    "    # model_id = \"cohere.command-r-v1:0\" # doesn't support tool use in streaming mode\n",
    "\n",
    "    system_prompt = \"\"\n",
    "\n",
    "    # Message to send to the model.\n",
    "    input_text = \"What is the most popular song on WZPZ and WZHK?\"\n",
    "    print(colored(f\"Question: {input_text}\", 'red'))\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    \n",
    "    # System prompts.\n",
    "    system_prompts = [{\"text\" : system_prompt}]\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.9\n",
    "    top_k = 200\n",
    "    max_tokens = 2000\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    try:\n",
    "        bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "        stream_conversation(bedrock_client, model_id, messages,\n",
    "                        system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "              format(message))\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nFinished streaming messages with model {model_id}.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2173bd93",
   "metadata": {},
   "source": [
    "## 非流式模式使用 Tool use 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac87298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to use tools with the Conversation API and the Claude model.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_text(bedrock_client, \n",
    "                  model_id, \n",
    "                  messages, \n",
    "                  system_prompts, \n",
    "                  inference_config, \n",
    "                  additional_model_fields, \n",
    "                  tool_config, \n",
    "                  round=0):\n",
    "    \"\"\"Generates text using the supplied Amazon Bedrock model. If necessary,\n",
    "    the function handles tool use requests and sends the result to the model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The Amazon Bedrock model ID.\n",
    "        tool_config (dict): The tool configuration.\n",
    "        messages (list): The input messages.\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating text with model %s\", model_id)\n",
    "\n",
    "\n",
    "    bedrock_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": inference_config,\n",
    "        \"additionalModelRequestFields\": additional_model_fields,\n",
    "        \"toolConfig\": tool_config\n",
    "    }\n",
    "\n",
    "    system = [item for item in system_prompts if item.get('text')]\n",
    "    if system:\n",
    "        bedrock_params['system'] = system\n",
    "\n",
    "\n",
    "    response = bedrock_client.converse(**bedrock_params)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "    output_content = output_message['content']\n",
    "\n",
    "    tool_list = []\n",
    "    tool_result_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": []\n",
    "    }\n",
    "\n",
    "    for item in output_content:\n",
    "        if 'text' in item:\n",
    "            print(colored(item['text'], 'green'))\n",
    "\n",
    "        if 'toolUse' in item:\n",
    "            tool_list.append(item['toolUse'])\n",
    "\n",
    "    for tool in tool_list:\n",
    "        logger.info(\"Requesting tool %s. Request: %s\",\n",
    "                        tool['name'], tool['toolUseId'])\n",
    "\n",
    "        tool_result = {}\n",
    "        try:\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool['toolUseId'],\n",
    "                \"content\": [ invoke_tool(tool) ]\n",
    "            }\n",
    "        except StationNotFoundError as err:\n",
    "            tool_result = {\n",
    "                \"toolUseId\": tool['toolUseId'],\n",
    "                \"content\": [{\"text\":  err.args[0]}],\n",
    "                \"status\": 'error'\n",
    "            }\n",
    "\n",
    "        tool_result_message['content'].append({\n",
    "            \"toolResult\": tool_result\n",
    "        })\n",
    "    if tool_list:\n",
    "        messages.append(tool_result_message)\n",
    "        generate_text(bedrock_client, model_id, messages, system_prompts, \n",
    "                      inference_config, additional_model_fields, tool_config, round + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ddb8aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mQuestion: What is the most popular song on WZPZ and WZHK? think step by step\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Requesting tool top_song. Request: tooluse_s5IXNXkzTg679cXcuGdL-Q\n",
      "INFO:__main__:Generating text with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mOkay, let's break this down step-by-step:\n",
      "\n",
      "1. To get the most popular song played on radio station WZPZ, we can use the `top_song` tool with the following parameters:\u001b[0m\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZPZ'}. id: tooluse_s5IXNXkzTg679cXcuGdL-Q\n",
      "Tool invoke id: tooluse_s5IXNXkzTg679cXcuGdL-Q, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Requesting tool top_song. Request: tooluse_JHVc9rxOTqSpmrdCmoIm2A\n",
      "INFO:__main__:Generating text with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mGot it, the most popular song on WZPZ is \"Elemental Hotel\" by 8 Storey Hike.\n",
      "\n",
      "2. To get the most popular song on radio station WZHK, we use the tool again with the new call sign:\u001b[0m\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZHK'}. id: tooluse_JHVc9rxOTqSpmrdCmoIm2A\n",
      "Tool invoke id: tooluse_JHVc9rxOTqSpmrdCmoIm2A, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n",
      "\u001b[32mOkay, the most popular song on both WZPZ and WZHK is \"Elemental Hotel\" by 8 Storey Hike.\n",
      "\n",
      "To summarize:\n",
      "The most popular song on WZPZ is \"Elemental Hotel\" by 8 Storey Hike.\n",
      "The most popular song on WZHK is also \"Elemental Hotel\" by 8 Storey Hike.\n",
      "\n",
      "It seems this song is currently very popular across different radio stations in the area. Let me know if you need any other details!\u001b[0m\n",
      "Finished generating text with model anthropic.claude-3-sonnet-20240229-v1:0.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for tool use example.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.9\n",
    "    top_k = 200\n",
    "    max_tokens = 2000\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    system_prompts = []\n",
    "    input_text = \"What is the most popular song on WZPZ and WZHK? think step by step\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }]\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    try:\n",
    "        print(colored(f\"Question: {input_text}\", 'red'))\n",
    "        generate_text(bedrock_client, model_id, messages,\n",
    "                        system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(f\"A client error occured: {message}\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text with model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93302f71",
   "metadata": {},
   "source": [
    "## 多轮对话示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716b5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "config = Config(read_timeout=1000) # second\n",
    "\n",
    "# model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# model_id = \"anthropic.claude-3-opus-20240229-v1:0\" \n",
    "model_id = \"cohere.command-r-v1:0\" # doesn't support tool use in streaming mode\n",
    "\n",
    "# inference parameters to use.\n",
    "temperature = 0.9\n",
    "top_k = 200\n",
    "max_tokens = 2000\n",
    "# Base inference parameters.\n",
    "inference_config = {\n",
    "    \"temperature\": temperature,\n",
    "    \"maxTokens\": max_tokens,\n",
    "}\n",
    "\n",
    "if model_id.startswith('anthropic.'):\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "else:\n",
    "    additional_model_fields = {}\n",
    "\n",
    "class HM:\n",
    "    def __init__(self, system_prompt=[]):\n",
    "        self.msgs = []\n",
    "        self.max_retries = 1\n",
    "        self.current_num_retries = 0\n",
    "        self.bedrock_client = boto3.client(service_name='bedrock-runtime', config=config)\n",
    "        self.system_prompts = [{\"text\": sp for sp in system_prompt if sp}]\n",
    "\n",
    "    def _put_user(self, user):\n",
    "        if user:\n",
    "            self.msgs.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user}]\n",
    "            })\n",
    "\n",
    "    def _put_assist(self, assist):\n",
    "        if assist:\n",
    "            self.msgs.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"text\": assist}]\n",
    "            })\n",
    "\n",
    "    def chat(self, user, assistant=None, stream=True):\n",
    "        self._put_user(user)\n",
    "        self._put_assist(assistant)\n",
    "\n",
    "        try:\n",
    "            if stream:\n",
    "                stream_conversation(self.bedrock_client, model_id, self.msgs,\n",
    "                            self.system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "            else:\n",
    "                generate_text(self.bedrock_client, model_id, self.msgs,\n",
    "                            self.system_prompts, inference_config, additional_model_fields, tool_config)\n",
    "        except ClientError as err:\n",
    "            message = err.response['Error']['Message']\n",
    "            logger.error(\"A client error occurred: %s\", message)\n",
    "            print(\"A client error occured: \" +\n",
    "                format(message))\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\nFinished streaming messages with model {model_id}.\")\n",
    "\n",
    "    def print_msg(self):\n",
    "        print(json.dumps(self.msgs, ensure_ascii=False, indent=2))\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.msgs.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51c7fc",
   "metadata": {},
   "source": [
    "### Claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f642ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:A client error occurred: This model doesn't support tool use in streaming mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: This model doesn't support tool use in streaming mode.\n",
      "\n",
      "************ ROUND 0 START ************\n",
      "Streaming messages with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:A client error occurred: This model doesn't support tool use in streaming mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: This model doesn't support tool use in streaming mode.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the most popular song on WZPZ and WZHK?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"北京天气如何?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "hm_1 = HM()\n",
    "hm_1.chat(\"What is the most popular song on WZPZ and WZHK?\")\n",
    "hm_1.chat(\"北京天气如何?\")\n",
    "hm_1.print_msg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4558f33",
   "metadata": {},
   "source": [
    "### Cohere Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997a8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n",
      "INFO:__main__:Requesting tool top_song. Request: tooluse_PP11vdEPSjKewrFPt2WGig\n",
      "INFO:__main__:Requesting tool top_song. Request: tooluse_zKBi1vNWTPmmOL_PBl325w\n",
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI will run concurrent searches for each radio station to find their most popular songs, and relay this information to the user.\u001b[0m\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZPZ'}. id: tooluse_PP11vdEPSjKewrFPt2WGig\n",
      "Tool invoke id: tooluse_PP11vdEPSjKewrFPt2WGig, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n",
      "\n",
      "Invoking tool: top_song. params: {'sign': 'WZHK'}. id: tooluse_zKBi1vNWTPmmOL_PBl325w\n",
      "Tool invoke id: tooluse_zKBi1vNWTPmmOL_PBl325w, result: {'json': {'song': 'Elemental Hotel', 'artist': '8 Storey Hike'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThe most popular song on both WZPZ and WZHK is Elemental Hotel by 8 Storey Hike.\u001b[0m\n",
      "\n",
      "Finished streaming messages with model cohere.command-r-v1:0.\n",
      "\u001b[32mI'm afraid I can't find any information about the weather in Beijing. Sorry about that!\u001b[0m\n",
      "\n",
      "Finished streaming messages with model cohere.command-r-v1:0.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the most popular song on WZPZ and WZHK?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"I will run concurrent searches for each radio station to find their most popular songs, and relay this information to the user.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_PP11vdEPSjKewrFPt2WGig\",\n",
      "          \"name\": \"top_song\",\n",
      "          \"input\": {\n",
      "            \"sign\": \"WZPZ\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_zKBi1vNWTPmmOL_PBl325w\",\n",
      "          \"name\": \"top_song\",\n",
      "          \"input\": {\n",
      "            \"sign\": \"WZHK\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_PP11vdEPSjKewrFPt2WGig\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"json\": {\n",
      "                \"song\": \"Elemental Hotel\",\n",
      "                \"artist\": \"8 Storey Hike\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_zKBi1vNWTPmmOL_PBl325w\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"json\": {\n",
      "                \"song\": \"Elemental Hotel\",\n",
      "                \"artist\": \"8 Storey Hike\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The most popular song on both WZPZ and WZHK is Elemental Hotel by 8 Storey Hike.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"北京天气如何?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"I'm afraid I can't find any information about the weather in Beijing. Sorry about that!\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "hm_2 = HM()\n",
    "hm_2.chat(\"What is the most popular song on WZPZ and WZHK?\", None, False)\n",
    "hm_2.chat(\"北京天气如何?\", None, False)\n",
    "hm_2.print_msg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
